# DealScraper

## Overview

DealScraper is a comprehensive tool designed for business acquisition deal sourcing and valuation. It automatically aggregates business listings from multiple marketplaces including BizBuySell, BusinessExits, and Dealonomy, and provides a user-friendly front-end platform for viewing, analyzing, and tracking potential acquisition opportunities.

## Features

- **Multi-Source Data Aggregation**: Automatically scrapes business listings from BizBuySell, BusinessExits, and Dealonomy
- **Intelligent Filtering**: Sort and filter deals based on industry, price range, revenue, EBITDA, and other key metrics
- **Valuation Tools**: Built-in calculators for common valuation methods (revenue multiple, EBITDA multiple) and ChatGPT analysis of deals based on predetermined evaluation criteria.
- **Deal Tracking**: Save, categorize, and track your favorite business opportunities
- **Custom Alerts**: Set up notifications for new listings that match your criteria
- **Front-End Dashboard**: Intuitive web interface for browsing and analyzing deals

## Data Sources

- **BizBuySell**: America's largest business-for-sale marketplace
- **BusinessExits**: Online marketplace for buying and selling online businesses
- **Dealonomy**: Platform specializing in internet and technology business listings

## Architecture

DealScraper consists of two main components:

1. **Backend Scraper System**:
   - Individual web scrapers for each marketplace source (BizBuySell, BusinessExits, Dealonomy)
   - Data processing and extraction to CSV files
   - Each scraper operates independently and outputs structured data

2. **Front-End Deal Viewer**:
   - User-friendly dashboard for browsing deals
   - Data loaded directly from scraper output files
   - Advanced filtering and search capabilities
   - Valuation tools and calculators
   - Deal tracking and management features

## Setup and Installation

### Prerequisites

- Python 3.7+
- OpenAI API Key (for deal analysis)
- ScraperAPI Key (for web scraping)

### Backend Setup

1. Clone the repository:
   ```
   git clone https://github.com/yourusername/DealScraper.git
   cd DealScraper
   ```

2. Set up a virtual environment and install dependencies:
   ```
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   pip install -r requirements.txt
   ```

3. Configure your database settings in `config.py`

4. Run the initial setup script:
   ```
   python setup.py
   ```

5. Update API keys in the scraper files:

   **BizBuySell Scraper**
   - Open `bizbuysell/bizbuyselldealscraper.py` and update the following variables:
     ```python
     openai_key = "your_openai_api_key_here"  # Required for ChatGPT analysis
     scraperapi_key = "your_scraperapi_key_here"  # Required for web scraping
     ```

   **BusinessExits Scraper**
   - Open `businessexits/businessexitsscraper.py` and update the OpenAI API key if needed (around line 381):
     ```python
     openai_key = "your_openai_api_key_here"  # Required for region classification
     ```


### Front-End Setup (Streamlit)

1. Navigate to the DealViewer directory:
   ```
   cd DealViewer
   ```

2. Install Streamlit and dependencies:
   ```
   pip install -r requirements.txt
   # or
   pip install streamlit pandas numpy
   ```

3. Start the Streamlit server:
   ```
   streamlit run app.py
   ```

## Usage

### Running the Scrapers

Run each scraper individually to collect data from the different sources:

```
# Run BizBuySell scraper
python bizbuysell/bizbuyselldealscraper.py

# Run BusinessExits scraper
python businessexits/businessexitsscraper.py

# Run Dealonomy scraper
python dealonomy/dealonomyscraper.py
```

### Viewing the Dashboard

Access the Streamlit dashboard by navigating to `http://localhost:8501` in your web browser after starting the Streamlit server with the command above. The dashboard will load the data directly from the CSV files generated by the scrapers.

## Legal Considerations

Please note that web scraping may be subject to terms of service for each marketplace. Ensure you're complying with each site's policies, rate limits, and terms of use.

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the LICENSE file for details.