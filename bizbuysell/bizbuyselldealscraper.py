# -*- coding: utf-8 -*-
"""BizBuySellDealScraper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vf0D0Al6X9SFcOvF0jLtBkBUz2Cnp2sx

# RUN SCRAPES
"""

from pythonjsonlogger import jsonlogger
from urllib.parse import urlparse
import requests
import logging
import time
import random
import os
import math
import pandas as pd
from bs4 import BeautifulSoup
import json
import numpy as np
import openai
import re


openai_key = "" #INSERT OPENAI KEY HERE
client = openai.OpenAI(api_key=openai_key)


logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
logHandler = logging.StreamHandler()
formatter = jsonlogger.JsonFormatter()
logHandler.setFormatter(formatter)
logger.addHandler(logHandler)


def runRequest(url, scraperapi=False):
    if (scraperapi):
        start_time = time.perf_counter()
        payload = { 'api_key': '5a4fe9277d64dee6d98516137342135c', 'url': url }
        response = requests.get('https://api.scraperapi.com/', params=payload)
        end_time = time.perf_counter()
        elapsed_time = end_time - start_time
        status = response.status_code
        return(response, status, elapsed_time)
    else:
        safari_agents = [
            'Safari/17612.3.14.1.6 CFNetwork/1327.0.4 Darwin/21.2.0',
        ]
        user_agent = random.choice(safari_agents)
        headers = {
            'User-Agent': user_agent
        }
        proxy = None
        start_time = time.perf_counter()
        response = requests.get(url, headers=headers, proxies={'http': proxy, 'https': proxy})
        end_time = time.perf_counter()
        elapsed_time = end_time - start_time
        status = response.status_code
        return(response, status, elapsed_time)

def dispatch(url, state, filepath, date_added):
    response, status, elapsed_time = runRequest(url)

    if status != 200:
        dispatch(url, state, filepath, date_added)
    else:
        logger.info(
            msg=f"status={status}, url={url}",
            extra={
                "elapsed_time": f"{elapsed_time:.4f}",
            }
        )

        dir = f"./{filepath}"
        os.makedirs(dir, exist_ok=True)
        idx = url.split(f"https://www.bizbuysell.com/{state}-businesses-for-sale/")[-1]
        idx = idx.split(f"/")[0]
        if date_added == 'Last 3 Days':
            date = '-last-3-days'
        elif date_added == 'Last 7 Days':
            date = '-last-7-days'
        elif date_added == 'Last 30 Days':
            date = '-last-30-days'
        else:
            date = ''
        loc = f"{dir}/bizbuysell-{state}-{idx}{date}.html"

        with open(loc, mode="w", encoding="utf-8") as fd:
            fd.write(response.text)

def main(state, filepath, date_added):
    start_urls = []
    i = 1
    stop = False
    while stop == False:
        if date_added == 'Last 3 Days':
            url = f"https://www.bizbuysell.com/{state}-businesses-for-sale/{i}/?q=ZGxhPTM%3D"
        elif date_added == 'Last 7 Days':
            url = f"https://www.bizbuysell.com/{state}-businesses-for-sale/{i}/?q=ZGxhPTc%3D"
        elif date_added == 'Last 30 Days':
            url = f"https://www.bizbuysell.com/{state}-businesses-for-sale/{i}/?q=ZGxhPTMw"
        else:
            url = f"https://www.bizbuysell.com/{state}-businesses-for-sale/{i}"
        start_urls.append(url)
        dispatch(url, state, filepath, date_added)

        if i == 1:
            response, status, elapsed_time = runRequest(url)
            page_count = str(response.text)
            try:
                page_count = page_count[page_count.rindex('title="Page') + 12:]
                page_count = int(page_count[:page_count.index('"')])
            except:
                page_count = page_count[:page_count.index(' results')]
                page_count = page_count[page_count.rindex('"') + 1:]
                page_count = math.ceil(int(page_count) / 50)

        if i == page_count:
            stop = True
        else:
            i = i + 1
    print(f"Total requests: {len(start_urls)}")

def scrape_file(filepath):
    with open(filepath, "r", encoding="utf-8") as file:
        html_content = file.read()
    soup = BeautifulSoup(html_content, "html.parser")
    json_ld_script = soup.find("script", type="application/ld+json")
    json_ld_data = json_ld_script.string
    json_data = json.loads(json_ld_data)
    return(json_data['about'])

def get_files(main_filepath, list_of_filepaths):
    list_of_files = []
    for filepath in list_of_filepaths:
        list_of_files.append(scrape_file(f"/content/{main_filepath}/{filepath}"))
    return(list_of_files)

def combine_lists(list_of_files):
    combined_list = []
    for sublist in list_of_files:
        for item in sublist:
            item.pop("position", None)
            combined_list.append(item)
    return(combined_list)

def get_data(combined_list, state, date_added):
    state = state.replace('-', ' ').title()

    data = []
    for entry in combined_list:
        item = entry["item"]
        offers = item.get("offers", {})

        data.append({
            "Name": item.get("name"),
            "Description": item.get("description"),
            "URL": item.get("url"),
            "Price": offers.get("price"),
            "Currency": offers.get("priceCurrency"),
            "Product ID": item.get("productId"),
            "State": state,
            "Date Added": date_added
        })
        df = pd.DataFrame(data)
    return(df)

def scrape_bizbuysell(state, filepath, run_scrape, date_added='All'):
    if run_scrape == True:
        state = state.replace(' ', '-')
        state = state.lower()
        main(state, filepath, date_added)
    list_of_filepaths = os.listdir(f'/content/{filepath}')
    try:
        list_of_filepaths.remove('.ipynb_checkpoints')
    except:
        list_of_filepaths = list_of_filepaths
    files = get_files(filepath, list_of_filepaths)
    combined_list = combine_lists(files)
    bizbuysell_df = get_data(combined_list, state, date_added)
    return(bizbuysell_df)

columns = ["Name", "Revenue", "Purchase Price", "Currency", "SDE", "Cash Flow", "EBITDA", "Margin", "Multiple", "URL", "Product ID", "City", "State", "Number of Employees", "Date Added", "Year Established", "Listed By"]

def clean_scrape_df(anytime_df, threeday_df, sevenday_df, thirtyday_df):
    final_df = threeday_df.copy()
    final_df_urls = set(final_df['URL'])
    sevenday_filtered = sevenday_df[~sevenday_df['URL'].isin(final_df_urls)]
    final_df = pd.concat([final_df, sevenday_filtered], ignore_index=True)
    final_df_urls.update(sevenday_filtered['URL'])
    thirtyday_filtered = thirtyday_df[~thirtyday_df['URL'].isin(final_df_urls)]
    final_df = pd.concat([final_df, thirtyday_filtered], ignore_index=True)
    final_df_urls.update(thirtyday_filtered['URL'])
    anytime_filtered = anytime_df[~anytime_df['URL'].isin(final_df_urls)]
    final_df = pd.concat([final_df, anytime_filtered], ignore_index=True)
    final_df = final_df.drop_duplicates(subset=['URL'], keep='first').reset_index(drop=True)

    final_df['Revenue'] = ''
    final_df['Purchase Price'] = final_df['Price']
    final_df['SDE'] = ''
    final_df['Cash Flow'] = ''
    final_df['EBITDA'] = ''
    final_df['Margin'] = ''
    final_df['Multiple'] = ''
    final_df['City'] = ''
    final_df['Number of Employees'] = ''
    final_df['Year Established'] = ''
    final_df['Listed By'] = ''

    final_df = final_df[columns]

    return(final_df)

def get_final_df(state, run_scrape):
    anytime_df = scrape_bizbuysell(state, 'bizbuysell-all', run_scrape, date_added='Added Anytime')
    threeday_df = scrape_bizbuysell(state, 'bizbuysell-3days', run_scrape, date_added='Last 3 Days')
    sevenday_df = scrape_bizbuysell(state, 'bizbuysell-7days', run_scrape, date_added='Last 7 Days')
    thirtyday_df = scrape_bizbuysell(state, 'bizbuysell-30days', run_scrape, date_added='Last 30 Days')
    final_df = clean_scrape_df(anytime_df, threeday_df, sevenday_df, thirtyday_df)
    return(final_df)

def get_html_data(url, scraperapi=False):
    response, status, elapsed_time = runRequest(url, scraperapi)
    soup = BeautifulSoup(response.text, "html.parser")
    data = soup.find(class_="pageContent fullPage")
    listing_financials_html = data.find(class_='financials')
    listing_description_html = data.find(class_='businessDescription')
    listing_details_html = data.find(class_='listingProfile_details row f-m row-g8')
    return(listing_financials_html, listing_description_html, listing_details_html, status, elapsed_time)

def get_dictionary(final_df):
    final_dictionary = final_df.to_dict(orient='records')
    length = len(final_dictionary)
    i = 0
    while i < length:
        if '/business-broker/' in final_dictionary[i]['URL']:
            removed_val = final_dictionary.pop(i)
            i = i - 1
            length = length - 1
        i = i + 1
    return(final_dictionary)

def scrape_listing_details(final_df, list_of_i_values=[]):
    final_dictionary = get_dictionary(final_df)
    if list_of_i_values == []:
        list_of_i_values = range(len(final_dictionary))
    FAILED_URLS = []
    for i in list_of_i_values:
        url = final_dictionary[i]['URL']
        print(str(i) + " out of " + str(len(final_dictionary)) + ": " + url)
        try:
            if "Business-Auction" in url:
                listing_dictionary = {"Listing Financials HTML" : '', "Listing Description HTML" : '', "Listing Details HTML" : '', "Type of Listing" : 'Auction'}
            elif "franchise-for-sale" in url:
                listing_dictionary = {"Listing Financials HTML" : '', "Listing Description HTML" : '', "Listing Details HTML" : '', "Type of Listing" : 'Franchise'}
            else:
                try:
                    listing_financials_html, listing_description_html, listing_details_html, status, elapsed_time = get_html_data(url)
                except:
                    listing_financials_html, listing_description_html, listing_details_html, status, elapsed_time = get_html_data(url, True)
                listing_dictionary = {"Listing Financials HTML" : listing_financials_html, "Listing Description HTML" : listing_description_html, "Listing Details HTML" : listing_details_html}
                if "Business-Opportunity" in url:
                    listing_dictionary.update({"Type of Listing" : 'Business Opportunity'})
                elif "Business-Real-Estate-For-Sale" in url:
                    listing_dictionary.update({"Type of Listing" : 'Business-Real-Estate-For-Sale'})
                elif "Business-Asset" in url:
                    listing_dictionary.update({"Type of Listing" : 'Business-Asset'})
                elif "Start-Up-Business" in url:
                    listing_dictionary.update({"Type of Listing" : 'Start-Up-Business'})
                elif "Business-Real-Estate-For-Lease" in url:
                    listing_dictionary.update({"Type of Listing" : 'Business-Real-Estate-For-Lease'})
                else:
                    listing_dictionary.update({"Type of Listing" : ''})
                print(str(status) + ' ' + str(elapsed_time))
                time.sleep(1)
            final_dictionary[i].update(listing_dictionary)
        except:
            listing_dictionary = {"Listing Financials HTML" : '', "Listing Description HTML" : '', "Listing Details HTML" : '', "Type of Listing" : ''}
            final_dictionary[i].update(listing_dictionary)
            FAILED_URLS.append(url)
    return(final_dictionary, FAILED_URLS)

"""RUNNING FUNCTIONS:"""

# final_df = get_final_df('Texas', False)
# final_dictionary, FAILED_URLS = scrape_listing_details(final_df)

# end_df = pd.DataFrame.from_records(final_dictionary)
# end_df.to_csv('end_df.csv')

end_df = pd.read_csv('end_df.csv')

def extract_financials(x):
    try:
        soup = BeautifulSoup(x, "html.parser")
        financials = {}
        p_tags = soup.select("p.m-listing-row")
        for p in p_tags:
            title_tag = p.find("span", class_="title")
            value_tag = p.find("span", class_="normal")
            if title_tag and value_tag:
                title = title_tag.get_text(strip=True)
                value = value_tag.get_text(strip=True)
                financials[title] = value
        return(financials)
    except:
        return({})

end_df['Listing Financials'] = end_df['Listing Financials HTML'].apply(lambda x: extract_financials(x))

def extract_description(x):
    try:
        soup = BeautifulSoup(x, "html.parser")
        description_tag = soup.find("div", class_="businessDescription")
        if description_tag:
            for br in description_tag.find_all("br"):
                br.replace_with("\n")

            hidden_p = description_tag.find("p")
            if hidden_p:
                hidden_p.decompose()

            description = description_tag.get_text(separator=" ", strip=True)
            return description
        else:
            return("")
    except:
        return("")

end_df['Listing Description'] = end_df['Listing Description HTML'].apply(lambda x: extract_description(x))

def extract_details(x):
    try:
        soup = BeautifulSoup(x, "html.parser")
        details = {}
        listing_sections = soup.find_all("dl", class_="listingProfile_details")
        for section in listing_sections:
            dt_tags = section.find_all("dt")
            dd_tags = section.find_all("dd")
            for dt, dd in zip(dt_tags, dd_tags):
                key = dt.get_text(strip=True)
                value = dd.get_text(separator=" ", strip=True)
                details[key] = value
        return details
    except:
        return({})

end_df['Listing Details'] = end_df['Listing Details HTML'].apply(lambda x: extract_details(x))

columns = list(end_df.columns)
columns.remove('Unnamed: 0')
columns.remove('Listing Financials HTML')
columns.remove('Listing Description HTML')
columns.remove('Listing Details HTML')
columns.remove('Type of Listing')
columns.append('Type of Listing')
bizbuyselldata = end_df[columns].copy()
bizbuyselldata.loc[:,'ChatGPT Brief Analysis'] = np.nan
bizbuyselldata.loc[:,'ChatGPT Rating'] = np.nan
bizbuyselldata.loc[:,'Industry'] = np.nan
bizbuyselldata.to_csv('bizbuyselldata_unscraped.csv')

"""# New ChatGPT Method"""

def get_prompt(listing_financials, listing_description, listing_details, acquisition_criteria):
    return f"""
    You are given three extracted fields from a business listing: Listing Financials, Listing Description, and Listing Details.

    Your task is to extract exactly the following values in this order:

    ['Industry', 'Revenue', 'Purchase Price', 'Currency', 'SDE', 'Cash Flow', 'EBITDA', 'City', 'Number of Employees', 'Year Established', 'Listed By', 'ChatGPT Brief Analysis', 'ChatGPT Rating']

    RULES:
    - Output must be a **single JSON array** with exactly 12 elements.
    - Output **must NOT** contain any additional text, notes, comments, or explanations outside the array.
    - Do not preface, explain, or summarize anything. Only the array must be output.
    - All values must be strings, except for the last value (ChatGPT Rating), which must be a plain integer without quotes.
    - If a value is missing or cannot be inferred, return an empty string "" for that value.
    - Revenue, Purchase Price, SDE, Cash Flow, and EBITDA should be numbers without currency symbols (e.g., 300000, not "$300,000").
    - For Currency, infer from symbols if possible ("$" → "USD").
    - City must only be the city name (no state abbreviation).
    - Industry must be inferred based on the listing description. If cannot be predicted, industry should be labeled as "".
    - 
    - For ChatGPT Brief Analysis: Write exactly 3 sentences assessing the business as an acquisition target according to the Cayuga Lake Capital acquisition criteria.
    - For ChatGPT Rating: Provide a single integer from 1 to 10.

    Cayuga Lake Capital Acquisition Criteria:
    - {acquisition_criteria[0]}
    - {acquisition_criteria[1]}
    - {acquisition_criteria[2]}

    Input:

    Listing Financials: {listing_financials}

    Listing Description: {listing_description}

    Listing Details: {listing_details}

    EXAMPLE Output format:

    [
    "Industry value here",
    "Revenue value here",
    "Purchase Price value here",
    "Currency value here",
    "SDE value here",
    "Cash Flow value here",
    "EBITDA value here",
    "City value here",
    "Number of Employees value here",
    "Year Established value here",
    "Listed By value here",
    "Brief Analysis (exactly 3 sentences)",
    Rating (integer 1-10)
    ]
    """


all_columns = ['Name', 'Industry', 'Revenue', 'Purchase Price', 'Currency', 'SDE', 'Cash Flow', 'EBITDA', 'Margin', 'Multiple', 'URL', 'Product ID', 'City', 'State', 'Number of Employees', 'Date Added',
               'Year Established', 'Listed By', 'Listing Financials', 'Listing Description', 'Listing Details', 'Type of Listing', 'ChatGPT Brief Analysis', 'ChatGPT Rating']

def query_chatgpt(prompt):
    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "You are an expert in business analysis."},
                {"role": "user", "content": prompt}
            ]
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"Error querying OpenAI: {e}")
        return ""

def run_chatgpt_call(df, columns_to_fill):
    df = df.reset_index(drop=True)

def calculate_margin(cash_flow, revenue):
    if cash_flow == '' or revenue == '' or revenue == "0" or revenue == 0:
        return ''
    else:
        return round(float(cash_flow) / float(revenue), 4)

def calculate_multiple(purchase_price, cash_flow):
    if purchase_price == '' or cash_flow == '' or cash_flow == "0" or cash_flow == 0:
        return ''
    else:
        return round(float(purchase_price) / float(cash_flow), 4)


def parse_to_int(value):
    if pd.isna(value):
        return np.nan

    value = str(value).strip().lower()

    # Match a numeric range like "1-10" or "20 – 30"
    match = re.search(r'(\d+)\s*[-–]\s*(\d+)', value)
    if match:
        low, high = int(match.group(1)), int(match.group(2))
        return (low + high) // 2

    # Match "Over 10", "More than 10", or "10+"
    match = re.search(r'(over|more than)\s*(\d+)', value)
    if match:
        return int(match.group(2))

    match = re.search(r'(\d+)\s*\+', value)
    if match:
        return int(match.group(1))

    # Match "Less than 5"
    match = re.search(r'less than\s*(\d+)', value)
    if match:
        return int(match.group(1)) - 1  # or just int(match.group(1))

    # Match a standalone integer
    match = re.search(r'\d+', value)
    if match:
        return int(match.group(0))

    return np.nan





def clean_chatgpt_df(df):
    df = df[all_columns]
    df = df.replace('""', '')
    df['Cash Flow'] = df['Cash Flow'].apply(lambda x: str(x).replace(',',''))
    df['Revenue'] = df['Revenue'].apply(lambda x: str(x).replace(',',''))
    df['Purchase Price'] = df['Revenue'].apply(lambda x: str(x).replace(',',''))
    df['Margin'] = df.apply(lambda x: calculate_margin(x['Cash Flow'], x['Revenue']), axis=1)
    df['Multiple'] = df.apply(lambda x: calculate_margin(x['Purchase Price'], x['Cash Flow']), axis=1)
    df['Year Established'] = df['Year Established'].apply(lambda x: parse_to_int(x) if x != '' else np.nan)
    df['Number of Employees'] = df['Number of Employees'].apply(lambda x: parse_to_int(x) if x != '' else np.nan)
    df['ChatGPT Rating'] = df['ChatGPT Rating'].apply(lambda x: parse_to_int(x) if x != '' and x!= 0 else np.nan)
    return(df)

cayuga_lake_capital_acquisition_criteria = ["Financial: $2m to $20m in revenue; $500k to $2m in seller earnings; Low capital expenditures",
                                           "Company: Profitable, with stable operating history of 5+ years; Recurring or repeat revenues and/or loyal customers; Preference for non-cyclical services in fragmented industries",
                                           "Owner Profile: Motivated and seeking to retire, reduce day-to-day involvement or transition into a new role; Looking for a full exit, or almost full (we can discuss)"]



i = 0
def get_chatgpt_result(x, criteria):
    global i
    prompt = get_prompt(x['Listing Financials'], x['Listing Description'], x['Listing Details'], criteria)
    response = query_chatgpt(prompt)
    response = response.replace('\n', '').strip()
    response = response.replace('N/A', '')
    response = response.replace('NA', '')
    print(i)
    print(x['Name'])
    print(response)
    i += 1
    try:
        response = json.loads(response)
        return(response)
    except Exception as e:
        print(f"Error parsing response: {response}")
        return(['', '', '', '', '', '', '', '', '', '', '', '', ''])

final_columns = ["Industry", "Revenue", "Purchase Price", "Currency", "SDE", "Cash Flow", "EBITDA", "City", "Number of Employees", "Year Established", "Listed By", "ChatGPT Brief Analysis", "ChatGPT Rating"]
listing_types = ['Business Opportunity', 'Business-Real-Estate-For-Sale', 'Business-Asset', 'Start-Up-Business']
def get_chatgpt_results(df, columns, listing_types, start_i, end_i):
    global i
    i = start_i
    df = df.loc[df['Type of Listing'].isin(listing_types)].reset_index(drop=True).copy()
    final_data = df.iloc[start_i:end_i + 1].copy()
    final_data[columns] = final_data.apply(lambda x:pd.Series(get_chatgpt_result(x,cayuga_lake_capital_acquisition_criteria)), axis=1)
    final_data = clean_chatgpt_df(final_data)
    print(final_data)
    final_data.to_csv(f"finalbizbuyselldata_{start_i}_to_{end_i}.csv")

get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 0, 100)
get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 101, 200)
get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 201, 300)
get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 301, 400)
get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 401, 500)
get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 501, 600)
get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 601, 700)
get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 701, 800)
get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 801, 900)
get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 901, 1000)
get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 1001, 1100)
get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 1101, 1200)
get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 1201, 1300)
get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 1301, 1400)
get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 1401, 1500)
get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 1501, 1600)
get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 1601, 1629)
# get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 1630, 1700)
# get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 1701, 1800)
# get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 1801, 1900)
# get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 1901, 2000)
# get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 2001, 2100)
# get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 2101, 2200)
# get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 2201, 2300)
# get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 2301, 2400)
# get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 2401, 2500)
# get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 2501, 2600)
# get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 2601, 2700)
# get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 2701, 2800)
# get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 2801, 2900)
# get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 2901, 3000)
# get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 3001, 3500)
# get_chatgpt_results(bizbuyselldata, final_columns, listing_types, 3501, 3648)